# -*- coding: utf-8 -*-
"""[Code]-Deploy-RAG-Chatbot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RIqEgrFcSYTO6rlUj1jLoUuJFrtpZy4X

# RAG Chatbot
"""

!nvidia-smi

"""## **Install and import libraries**"""

!pip install -q transformers==4.52.4
!pip install -q bitsandbytes==0.46.0
!pip install -q accelerate==1.7.0
!pip install -q langchain==0.3.25
!pip install -q langchainhub==0.1.21
!pip install -q langchain-chroma==0.2.4
!pip install -q langchain_experimental==0.3.4
!pip install -q langchain-community==0.3.24
!pip install -q langchain_huggingface==0.2.0
!pip install -q python-dotenv==1.1.0
!pip install -q pypdf
!pip install -q streamlit==1.46.0

!curl https://loca.lt/mytunnelpassword

# Commented out IPython magic to ensure Python compatibility.
# %%writefile chatbot.py
# import streamlit as st
# import os
# import torch
# import requests
# from pathlib import Path
# from langchain_community.document_loaders import PyPDFLoader
# from langchain_huggingface.embeddings import HuggingFaceEmbeddings
# from langchain_experimental.text_splitter import SemanticChunker
# from langchain_chroma import Chroma
# from langchain_huggingface.llms import HuggingFacePipeline
# from langchain import hub
# from langchain_core.output_parsers import StrOutputParser
# from langchain_core.runnables import RunnablePassthrough
# from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
# from transformers import BitsAndBytesConfig
# import time
# import tempfile
# import urllib.parse
# 
# st.set_page_config(
#     page_title = "PDF RAG Assistant",
#     layout = "wide",
#     initial_sidebar_state = "expanded"
# )
# 
# st.markdown("""
# <style>
#   .main-header{
#     text-align: center;
#     padding: 1rem 0;
#     margin-bottom: 2rem;
#   }
#   .chat-container{
#     max-width: 800px;
#     margin: 0 auto;
#     padding: 1rem;
#     max-height: 500px;
#     overflow-y: auto;
#     border: 1px solid #e0e0e0;
#     border-radius: 10px;
#     margin-bottom: 20px;
#   }
#   .user-message{
#     background-color: #003300;
#     border-radius: 18px;
#     padding: 12px 16px;
#     margin: 8px 0;
#     margin-left: 20%;
#     text-align: left;
#     border: 1px solid #2196f3;
#   }
#   .assistant-message{
#     background-color: #000000;
#     border-radius: 18px;
#     padding: 12px 16px;
#     margin: 8px 0;
#     margin-right: 20%;
#     text-align: left;
#     border: 1px solid #4caf50;
#   }
#   .chat-input-container {
#     position: sticky;
#     bottom: 0;
#     background-color: white;
#     padding: 1rem;
#     border-top: 2px solid #e0e0e0;
#     border-radius: 10px;
#     margin-top: 20px;
#   }
#   .stTextInput > div > div > input {
#     border-radius: 25px;
#     border: 2px solid #e0e0e0;
#     padding: 12px 20px;
#     font-size: 16px;
#   }
#   .document-info {
#     background-color: #f8f9fa;
#     border-left: 4px solid #28a745;
#     padding: 1rem;
#     margin: 1rem 0;
#     border-radius: 4px;
#   }
#   .status-indicator{
#     display: inline-block;
#     width: 12px;
#     height: 12px;
#     border-radius: 50%;
#     margin-right: 8px;
#   }
#   .status-ready{
#     background-color: #28a745;
#   }
#   .status-loading {
#     background-color: #ffc107;
#   }
#   .status-error {
#     background-color: #dc3545;
#   }
#   .thinking-indicator {
#     background-color: #f5f5f5;
#     border-radius: 18px;
#     padding: 12px 16px;
#     margin: 8px 0;
#     margin-right: 20%;
#     text-align: left;
#     border: 1px solid #ddd;
#     animation: pulse 1.5s ease-in-out infinite;
#     }
# 
#     @keyframes pulse {
#     0% { opacity: 0.6; }
#     50% { opacity: 1; }
#     100% { opacity: 0.6; }
#     }
# </style>
# """, unsafe_allow_html=True)
# 
# # Session state initialization
# if 'chat_history' not in st.session_state:
#     st.session_state.chat_history = []
# if 'rag_chain' not in st.session_state:
#     st.session_state.rag_chain = None
# if 'models_loaded' not in st.session_state:
#     st.session_state.models_loaded = False
# if 'embeddings' not in st.session_state:
#     st.session_state.embeddings = None
# if 'llm' not in st.session_state:
#     st.session_state.llm = None
# if 'documents_loaded' not in st.session_state:
#     st.session_state.documents_loaded = False
# if 'pdf_source' not in st.session_state:
#     st.session_state.pdf_source = "github"  # Default to GitHub
# if 'github_repo_url' not in st.session_state:
#     st.session_state.github_repo_url = "https://github.com/Jennifer1907/Time-Series-Team-Hub/tree/main/assets/pdf"
# if 'local_folder_path' not in st.session_state:
#     st.session_state.local_folder_path = "./knowledge_base"
# if 'processing_query' not in st.session_state:
#     st.session_state.processing_query = False
# if 'query_input' not in st.session_state:
#     st.session_state.query_input = ""
# 
# @st.cache_resource
# def load_embeddings():
#   return HuggingFaceEmbeddings(model_name="bkai-foundation-models/vietnamese-bi-encoder")
# 
# @st.cache_resource
# def load_llm():
#   MODEL_NAME = "lmsys/vicuna-7b-v1.5"
# 
#   bnb_config = BitsAndBytesConfig(
#       load_in_4bit = True,
#       bnb_4bit_use_double_quant = True,
#       bnb_4bit_compute_dtype = torch.bfloat16,
#       bnb_4bit_quant_type = "nf4"
#   )
# 
#   model = AutoModelForCausalLM.from_pretrained(
#       MODEL_NAME,
#       quantization_config = bnb_config,
#       device_map = "auto"
#   )
# 
#   tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
# 
#   model_pipeline = pipeline(
#       "text-generation",
#       model = model,
#       tokenizer = tokenizer,
#       max_new_tokens = 512,
#       pad_token_id = tokenizer.eos_token_id,
#       device_map = "auto"
#   )
#   return HuggingFacePipeline(pipeline = model_pipeline)
# 
# def get_github_pdf_files(repo_url):
#   """
#   Link github hi·ªán t·∫°i l√† giao di·ªán d√†nh cho ng∆∞·ªùi d√πng (HTML), kh√¥ng ph·∫£i API d√†nh cho m√°y m√≥c.
#   ƒê·ªÉ ƒë·ªçc tr·ª±c ti·∫øp danh s√°ch file t·ª´ URL n√†y, ph·∫£i chuy·ªÉn link github hi·ªán t·∫°i sang API"""
#   try:
#     if "github.com" in repo_url and "/tree/" in repo_url:
#       parts = repo_url.replace("https://github.com/", "").split("/tree/")
#       repo_path = parts[0]
#       branch_and_path = parts[1].split("/",1)
#       branch = branch_and_path[0]
#       folder_path = branch_and_path[1] if len(branch_and_path)>1 else ""
# 
#       api_url = f"https://api.github.com/repos/{repo_path}/contents/{folder_path}?ref={branch}"
#     else:
#       st.error("Invalid GitHub URL format")
#       return[]
# 
#     response = requests.get(api_url)
#     if response.status_code == 200:
#       files = response.json()
#       pdf_files = []
#       for file in files:
#         if file['name'].endswith('.pdf') and file['type'] == 'file':
#           pdf_files.append({
#               'name': file['name'],
#               'download_url': file['download_url']
#           })
#       return pdf_files
#     else:
#       st.error(f"Failed to access GitHub repository: {response.status_code}")
#       return []
#   except Exception as e:
#     st.error(f"Error accessing GitHub repository: {str(e)}")
#     return []
# 
# def download_pdf_from_url(url, filename, temp_dir):
#   try:
#     response = requests.get(url)
#     if response.status_code == 200:
#       file_path = os.path.join(temp_dir, filename)
#       with open(file_path, 'wb') as f:
#         f.write(response.content)
#       return file_path
#     return None
#   except Exception as e:
#     st.error(f"Error downloading {filename}: {str(e)}")
#     return None
# 
# def load_pdfs_from_github(repo_url):
#   pdf_files = get_github_pdf_files(repo_url)
# 
#   if not pdf_files:
#     st.warning("No PDF files found in the GitHub repository")
#     return None, 0, []
# 
#   temp_dir = tempfile.mkdtemp()
#   all_documents = []
#   loaded_files = []
# 
#   progress_bar = st.progress(0)
#   status_text = st.empty()
# 
#   for i, pdf_file in enumerate(pdf_files):
#     try:
#       status_text.text(f"Downloading and processing: {pdf_file['name']}")
#       local_path = download_pdf_from_url(pdf_file['download_url'], pdf_file['name'], temp_dir)
# 
#       if local_path:
#         loader = PyPDFLoader(local_path)
#         documents = loader.load()
#         all_documents.extend(documents)
#         loaded_files.append(pdf_file['name'])
# 
#         st.success(f"‚úÖ Processed: {pdf_file['name']} ({len(documents)} pages)")
#       progress_bar.progress((i + 1) / len(pdf_files))
#     except Exception as e:
#       st.error(f"‚ùå Error processing {pdf_file['name']}: {str(e)}")
# 
#   progress_bar.empty()
#   status_text.empty()
# 
#   if not all_documents:
#     return None, 0, load_files
# 
#   semantic_splitter = SemanticChunker(
#       embeddings = st.session_state.embeddings,
#       buffer_size = 1,
#       breakpoint_threshold_type = "percentile",
#       breakpoint_threshold_amount = 95,
#       min_chunk_size = 500,
#       add_start_index = True
#   )
# 
#   docs = semantic_splitter.split_documents(all_documents)
#   vector_db = Chroma.from_documents(documents = docs, embedding = st.session_state.embeddings)
#   retriever = vector_db.as_retriever()
# 
#   prompt = hub.pull("rlm/rag-prompt")
# 
#   def format_docs(docs):
#     return "\n\n".join(doc.page_content for doc in docs)
#   rag_chain = (
#         {"context": retriever | format_docs, "question": RunnablePassthrough()}
#         | prompt
#         | st.session_state.llm
#         | StrOutputParser()
#     )
# 
# 
#   import shutil
#   shutil.rmtree(temp_dir)
# 
#   return rag_chain, len(docs), loaded_files
# 
# def load_pdfs_from_folder(folder_path):
#     """Load all PDF files from the specified local folder"""
#     cleaned_path = folder_path.strip().strip('"').strip("'")
#     folder = Path(cleaned_path)
# 
#     if not folder.exists():
#         st.error(f"‚ùå Folder does not exist: `{cleaned_path}`")
#         return None, 0, []
# 
#     pdf_files = list(folder.glob("*.pdf"))
#     if not pdf_files:
#         st.warning(f"No PDF files found in folder: {cleaned_path}")
#         return None, 0, []
# 
#     all_documents = []
#     loaded_files = []
# 
#     progress_bar = st.progress(0)
#     status_text = st.empty()
# 
#     for i, pdf_file in enumerate(pdf_files):
#         try:
#             status_text.text(f"Processing: {pdf_file.name}")
#             loader = PyPDFLoader(str(pdf_file))
#             documents = loader.load()
#             all_documents.extend(documents)
#             loaded_files.append(pdf_file.name)
#             progress_bar.progress((i + 1) / len(pdf_files))
#             st.success(f"‚úÖ Processed: {pdf_file.name} ({len(documents)} pages)")
# 
#         except Exception as e:
#             st.error(f"‚ùå Error processing {pdf_file.name}: {str(e)}")
# 
#     progress_bar.empty()
#     status_text.empty()
# 
#     if not all_documents:
#         return None, 0, loaded_files
# 
#     semantic_splitter = SemanticChunker(
#         embeddings=st.session_state.embeddings,
#         buffer_size=1,
#         breakpoint_threshold_type="percentile",
#         breakpoint_threshold_amount=95,
#         min_chunk_size=500,
#         add_start_index=True
#     )
# 
#     docs = semantic_splitter.split_documents(all_documents)
#     vector_db = Chroma.from_documents(documents=docs, embedding=st.session_state.embeddings)
#     retriever = vector_db.as_retriever()
# 
#     prompt = hub.pull("rlm/rag-prompt")
# 
#     def format_docs(docs):
#         return "\n\n".join(doc.page_content for doc in docs)
# 
#     rag_chain = (
#         {"context": retriever | format_docs, "question": RunnablePassthrough()}
#         | prompt
#         | st.session_state.llm
#         | StrOutputParser()
#     )
#     return rag_chain, len(docs), loaded_files
# 
# def display_chat_message(message, is_user = True):
#   if is_user:
#     st.markdown(f"""
#     <div class = "user-message">
#         <strong>You:</strong>{message}
#     </div>
#     """, unsafe_allow_html = True)
#   else:
#     st.markdown(f"""
#     <div class = "assistant-message">
#         <strong>AI Assistant:</strong>{message}
#     </div>
#     """, unsafe_allow_html = True)
# 
# def display_thinking_indicator():
#   st.markdown(f"""
#   <div class = "thinking-indicator">
#       <strong>AI Assistant:</strong> ü§î Thinking...
#     </div>
#     """, unsafe_allow_html=True)
# 
# def process_user_query(question):
#   try:
#     output = st.session_state.rag_chain.invoke(question)
#     answer = output.split('Answer:')[1].strip() if 'Answer:' in output else output.strip()
#     return answer
#   except Exception as e:
#     return f"Sorry, an error occurred while processing your question: {str(e)}"
# 
# def main():
#     # Header
#     st.markdown("""
#     <div class="main-header">
#         <h1>ü§ñ PDF RAG Assistant</h1>
#         <p>Smart AI Assistant - Q&A with AIO in Vietnamese</p>
#     </div>
#     """, unsafe_allow_html=True)
# 
#     with st.sidebar:
#       st.header("‚öôÔ∏è Configuration")
# 
#       if st.session_state.models_loaded:
#         st.markdown('<span class="status-indicator status-ready"></span>**Models:** Ready', unsafe_allow_html=True)
#       else:
#           st.markdown('<span class="status-indicator status-loading"></span>**Models:** Loading...', unsafe_allow_html=True)
# 
#       # Document loading status
#       if st.session_state.documents_loaded:
#           st.markdown('<span class="status-indicator status-ready"></span>**Documents:** Loaded', unsafe_allow_html=True)
#       else:
#           st.markdown('<span class="status-indicator status-error"></span>**Documents:** Not loaded', unsafe_allow_html=True)
# 
#       st.divider()
# 
#       # Document source selection
#       st.subheader("üìÅ Document Source")
# 
#       pdf_source = st.radio(
#             "Choose document source:",
#             ["GitHub Repository (Default)", "Local Folder"],
#             key="pdf_source_radio"
#         )
# 
#       if pdf_source == "GitHub Repository (Default)":
#           st.session_state.pdf_source = "github"
#           github_url = st.text_input(
#               "GitHub Repository URL:",
#               value=st.session_state.github_repo_url,
#               help="URL to GitHub folder containing PDF files"
#           )
#           st.session_state.github_repo_url = github_url
#       else:
#           st.session_state.pdf_source = "local"
#           local_path = st.text_input(
#               "Local Folder Path:",
#               value=st.session_state.local_folder_path,
#               help="Path to local folder containing PDF files"
#           )
#           st.session_state.local_folder_path = local_path
# 
#       if st.button("üîÑ Load Documents", type="primary"):
#           st.session_state.documents_loaded = False
#           st.rerun()
# 
#       if st.button("üóëÔ∏è Clear Chat History"):
#           st.session_state.chat_history = []
#           st.session_state.processing_query = False
#           st.rerun()
# 
#     if not st.session_state.models_loaded:
#         with st.spinner("üöÄ Initializing AI models..."):
#             st.session_state.embeddings = load_embeddings()
#             st.session_state.llm = load_llm()
#             st.session_state.models_loaded = True
#         st.success("‚úÖ Models ready!")
#         time.sleep(1)
#         st.rerun()
# 
#     if st.session_state.models_loaded and not st.session_state.documents_loaded:
#       with st.spinner("üìö Loading documents..."):
#         if st.session_state.pdf_source == "github":
#           rag_chain, num_chunks, loaded_files = load_pdfs_from_github(st.session_state.github_repo_url)
#         else:
#             rag_chain, num_chunks, loaded_files = load_pdfs_from_folder(st.session_state.local_folder_path)
# 
#         if rag_chain:
#             st.session_state.rag_chain = rag_chain
#             st.session_state.documents_loaded = True
# 
#             st.markdown(f"""
#             <div class="document-info">
#                 <h4>üìÑ Successfully loaded {len(loaded_files)} PDF documents:</h4>
#                 <ul>
#                     {"".join([f"<li>{file}</li>" for file in loaded_files])}
#                 </ul>
#                 <p><strong>Total chunks:</strong> {num_chunks}</p>
#             </div>
#             """, unsafe_allow_html=True)
# 
#             st.success("‚úÖ Documents ready for Q&A!")
#             time.sleep(2)
#             st.rerun()
#         else:
#             st.error("‚ùå Failed to load documents. Please check your configuration.")
# 
#     if st.session_state.rag_chain:
#       st.markdown("<div class = 'chat-container'>", unsafe_allow_html= True)
# 
#       for message in st.session_state.chat_history:
#         display_chat_message(message["content"], message["is_user"])
# 
#       if st.session_state.processing_query:
#         display_thinking_indicator()
# 
#       st.markdown("</div>", unsafe_allow_html = True)
# 
#       st.markdown("<div class = 'chat-input-container'>", unsafe_allow_html = True)
# 
#       with st.form(key ="chat_form", clear_on_submit=True):
#         col1, col2 = st.columns([4, 1])
# 
#         with col1:
#           user_question = st.text_input(
#                 "Type your question...",
#                 placeholder="Ask anything about the documents...",
#                 disabled=st.session_state.processing_query,
#                 label_visibility="collapsed"
#             )
# 
#         with col2:
#             send_button = st.form_submit_button(
#                 "üì§ Send",
#                 type="primary",
#                 disabled=st.session_state.processing_query
#             )
# 
#       st.markdown("</div>", unsafe_allow_html=True)
# 
#       # Process user input
#       if send_button and user_question.strip() and not st.session_state.processing_query:
#         st.session_state.processing_query = True
# 
#         st.session_state.chat_history.append({
#             "content": user_question,
#             "is_user": True
#         })
#         st.rerun()
# 
#       if st.session_state.processing_query and len(st.session_state.chat_history)>0:
#         if not st.session_state.chat_history[-1]["is_user"]:
#           st.session_state.processing_query = False
#         else:
#           last_question = st.session_state.chat_history[-1]["content"]
#           answer = process_user_query(last_question)
# 
#           st.session_state.chat_history.append({
#               "content": answer,
#               "is_user": False
#           })
# 
#           st.session_state.processing_query = False
# 
#           st.rerun()
#     else:
#       st.markdown("""
#       <div style='text-align: center; padding: 2rem;'>
#         <h3>üëã Welcome to PDF RAG Assistant!</h3>
#         <p>To get started, please:</p>
#         <ol style='text-align: left; max-width: 500px; margin: 0 auto;'>
#             <li><strong>Choose your document source:</strong>
#                 <ul>
#                     <li><strong>GitHub Repository (Default):</strong> Uses the pre-configured repository</li>
#                     <li><strong>Local Folder:</strong> Use documents from your local machine</li>
#                 </ul>
#             </li>
#             <li><strong>Click "Load Documents"</strong> to process the PDF files</li>
#             <li><strong>Start asking questions!</strong> The AI will answer based on your documents</li>
#         </ol>
#         <br>
#         <p><strong>Default Repository:</strong><br>
#         <code>https://github.com/Jennifer1907/Time-Series-Team-Hub/tree/main/assets/pdf</code></p>
#       </div>
#       """, unsafe_allow_html=True)
# 
# if __name__ == "__main__":
#     main()
# 
# 
# 
# 
# 
#

!streamlit run chatbot.py & npx localtunnel --port 8501